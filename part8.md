为了便于模型部署与推理，本实验将微调完成的操作系统大语言模型保存为 GGUF（Grokking GGML Universal Format）格式。GGUF 是一种高效通用的模型格式，支持多种量化方法，适用于在资源受限设备或本地环境中加载运行。

模型身份认证与准备
在模型保存之前，实验通过 Google Colab 的 Secret 功能调用了事先保存的 Hugging Face API Token，并对模型进行了加载与验证，确保模型和分词器对象均可正常使用。

模型保存与量化策略
我们尝试了多种 GGUF 格式的保存方式，最终选择使用默认的 8 位量化（Q8_0）进行模型导出。该格式在压缩模型体积的同时，仍保持良好的推理性能，适合部署于普通计算资源的设备中。

另外，还评估了两种备选的保存方案，包括：

使用 16 位浮点格式（f16）：具备更高的精度，但体积较大，未启用；

使用 4 位量化格式（q4_k_m）：压缩率最高，但可能牺牲一定推理精度，未启用。

通过灵活控制量化精度，可以针对不同应用场景进行部署优化。
